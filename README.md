# nlp-evaluation-metrics
This will contain notebooks I used to evaluate results which I generated with transformer models for Sinhalese Datasets

## MBartForSequenceClassification finetuned for Glue MRPC task
GLUE_MPRC.ipynb
{'eval_accuracy': 0.9731308411214953,
 'eval_f1': 0.9814963797264683,
 'eval_loss': 0.1659528613090515}
 but the results were not stable enough to use this model for evaluation tasks.
